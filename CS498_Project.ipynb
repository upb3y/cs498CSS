{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goals\n",
    "#analyze thread attributs & engagements\n",
    "#data: https://www.kaggle.com/datasets/danielgrijalvas/twitter-threads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64697d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   thread_number     retweets                likes            replies       \n",
      "                         mean  median         mean  median       mean median\n",
      "0       Thread 1    11.652174    10.0    25.782609    21.0   2.608696    2.0\n",
      "1      Thread 10    28.666667    21.5    20.125000    14.0   1.958333    2.0\n",
      "2     Thread 100   180.285714   113.0   589.095238   396.0  21.476190   14.0\n",
      "3      Thread 11    23.666667    21.0    23.761905    19.0   5.428571    3.0\n",
      "4      Thread 12    15.478261     1.0    36.000000     5.0   1.521739    1.0\n",
      "..           ...          ...     ...          ...     ...        ...    ...\n",
      "95     Thread 95     1.250000     0.0    13.291667    11.0   1.625000    1.0\n",
      "96     Thread 96   190.916667   163.5   352.958333   326.0   9.041667    3.5\n",
      "97     Thread 97  2162.619048  1077.0  5495.142857  3498.0  58.571429    0.0\n",
      "98     Thread 98   446.043478   269.0  1347.565217  1069.0  11.086957    0.0\n",
      "99     Thread 99   119.454545    80.0   688.000000   592.5  27.227273   11.0\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "   thread_number    retweets               likes            replies       \n",
      "                        mean median         mean  median       mean median\n",
      "0       Thread 1   88.433333   72.0   285.366667   244.5   6.433333    3.5\n",
      "1      Thread 10   10.962963   10.0     5.703704     4.0   1.407407    1.0\n",
      "2     Thread 100    8.269231    7.0    31.884615    32.5   1.538462    1.0\n",
      "3      Thread 11  721.793103  487.0  1271.379310  1012.0  13.931034    0.0\n",
      "4      Thread 12    3.346154    1.0    22.038462    16.0   2.115385    1.0\n",
      "..           ...         ...    ...          ...     ...        ...    ...\n",
      "95     Thread 95    1.344828    0.0   122.758621    92.0   4.896552    4.0\n",
      "96     Thread 96  123.000000   93.0   396.866667   306.5  10.000000    8.0\n",
      "97     Thread 97    4.607143    3.0    20.892857    16.0   1.714286    1.0\n",
      "98     Thread 98   30.466667    4.0   562.500000   415.5   9.000000    6.5\n",
      "99     Thread 99   28.689655    6.0   187.517241   132.0   8.068966    3.0\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "   thread_number    retweets              likes           replies       \n",
      "                        mean median        mean median       mean median\n",
      "0       Thread 1  136.500000  120.5  381.722222  364.5  19.000000   11.5\n",
      "1      Thread 10    7.300000    5.0   16.000000   14.0   1.900000    1.0\n",
      "2      Thread 11   80.850000   64.5  109.200000   92.0   3.950000    2.5\n",
      "3      Thread 12   17.944444   14.5   24.722222   20.0   1.722222    1.5\n",
      "4      Thread 13    1.058824    0.0    3.352941    2.0   1.058824    1.0\n",
      "..           ...         ...    ...         ...    ...        ...    ...\n",
      "94     Thread 95   41.000000   32.0   81.950000   48.5   4.050000    3.0\n",
      "95     Thread 96  100.176471   72.0  131.411765   90.0   4.176471    3.0\n",
      "96     Thread 97   27.631579   18.0   33.789474   29.0   1.842105    1.0\n",
      "97     Thread 98   20.941176   10.0  111.294118   75.0   3.647059    2.0\n",
      "98     Thread 99   17.473684   15.0   40.157895   37.0   2.105263    2.0\n",
      "\n",
      "[99 rows x 7 columns]\n",
      "   thread_number    retweets              likes           replies       \n",
      "                        mean median        mean median       mean median\n",
      "0       Thread 1  136.500000  120.5  381.722222  364.5  19.000000   11.5\n",
      "1      Thread 10    7.300000    5.0   16.000000   14.0   1.900000    1.0\n",
      "2      Thread 11   80.850000   64.5  109.200000   92.0   3.950000    2.5\n",
      "3      Thread 12   17.944444   14.5   24.722222   20.0   1.722222    1.5\n",
      "4      Thread 13    1.058824    0.0    3.352941    2.0   1.058824    1.0\n",
      "..           ...         ...    ...         ...    ...        ...    ...\n",
      "94     Thread 95   41.000000   32.0   81.950000   48.5   4.050000    3.0\n",
      "95     Thread 96  100.176471   72.0  131.411765   90.0   4.176471    3.0\n",
      "96     Thread 97   27.631579   18.0   33.789474   29.0   1.842105    1.0\n",
      "97     Thread 98   20.941176   10.0  111.294118   75.0   3.647059    2.0\n",
      "98     Thread 99   17.473684   15.0   40.157895   37.0   2.105263    2.0\n",
      "\n",
      "[99 rows x 7 columns]\n",
      "   thread_number    retweets              likes           replies       \n",
      "                        mean median        mean median       mean median\n",
      "0       Thread 1  136.500000  120.5  381.722222  364.5  19.000000   11.5\n",
      "1      Thread 10    7.300000    5.0   16.000000   14.0   1.900000    1.0\n",
      "2      Thread 11   80.850000   64.5  109.200000   92.0   3.950000    2.5\n",
      "3      Thread 12   17.944444   14.5   24.722222   20.0   1.722222    1.5\n",
      "4      Thread 13    1.058824    0.0    3.352941    2.0   1.058824    1.0\n",
      "..           ...         ...    ...         ...    ...        ...    ...\n",
      "94     Thread 95   41.000000   32.0   81.950000   48.5   4.050000    3.0\n",
      "95     Thread 96  100.176471   72.0  131.411765   90.0   4.176471    3.0\n",
      "96     Thread 97   27.631579   18.0   33.789474   29.0   1.842105    1.0\n",
      "97     Thread 98   20.941176   10.0  111.294118   75.0   3.647059    2.0\n",
      "98     Thread 99   17.473684   15.0   40.157895   37.0   2.105263    2.0\n",
      "\n",
      "[99 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjusted function to examine influence of thread_number and timestamp on engagement\n",
    "def examine_engagement(dataframe):\n",
    "    # Convert the 'timestamp' column to datetime\n",
    "    dataframe['timestamp'] = pd.to_datetime(dataframe['timestamp'])\n",
    "    \n",
    "    # Group by 'thread_number' and date part of 'timestamp', then calculate mean engagement metrics\n",
    "    grouped = dataframe.groupby(['thread_number']).agg({\n",
    "        'retweets': ['mean', 'median'],\n",
    "        'likes': ['mean', 'median'],\n",
    "        'replies': ['mean', 'median']\n",
    "    }).reset_index()\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "# Corrected file path to match the uploaded file\n",
    "thread2025 = \"C:/Users/Jackd\\Downloads/archive (1)/twenty_twentyfive.csv\"\n",
    "df2025 = pd.read_csv(thread2025, encoding='ISO-8859-1')\n",
    "engagement_summary_2025 = examine_engagement(df2025)\n",
    "\n",
    "thread2530 = \"C:/Users/Jackd/Downloads/archive (1)/twentyfive_thirty.csv\"\n",
    "df2530 = pd.read_csv(thread2530, encoding='ISO-8859-1')\n",
    "engagement_summary_2530 = examine_engagement(df2530)\n",
    "\n",
    "thread1520 = \"C:/Users/Jackd/Downloads/archive (1)/fifteen_twenty.csv\"\n",
    "df1520= pd.read_csv(thread1520, encoding='ISO-8859-1')\n",
    "engagement_summary_1520 = examine_engagement(df1520)\n",
    "\n",
    "thread0510 = \"C:/Users/Jackd/Downloads/archive (1)/five_ten.csv\"\n",
    "df0510 = pd.read_csv(thread1520, encoding='ISO-8859-1')\n",
    "engagement_summary_0510 = examine_engagement(df0510)\n",
    "\n",
    "thread1015 = \"C:/Users/Jackd/Downloads/archive (1)/ten_fifteen.csv\"\n",
    "df1015 = pd.read_csv(thread1520, encoding='ISO-8859-1')\n",
    "engagement_summary_1015 = examine_engagement(df1015)\n",
    "\n",
    "\n",
    "print(engagement_summary_2025)\n",
    "print(engagement_summary_2530)\n",
    "print(engagement_summary_1520)\n",
    "print(engagement_summary_0510)\n",
    "print(engagement_summary_1015)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2411c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jackd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jackd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jackd\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:/Users/Jackd/Downloads/archive (1)/twenty_twentyfive.csv\n",
      "Topic 0: democratic candidate vote way time district gop new national thing\n",
      "Topic 1: post think state thread criminal people end money want mueller\n",
      "Topic 2: unroll comment anon related current drop say politico demand el\n",
      "Topic 3: say work asked mean answer tell witness year fact president\n",
      "Topic 4: le people social right la incumbent medium bio actually look\n",
      "\n",
      "Engagement Summary for Thread 2025:\n",
      "   Topic_  retweets_mean  retweets_median  likes_mean  likes_median  \\\n",
      "0       0     117.688663             24.0  309.847716          63.0   \n",
      "1       1     149.932367             36.0  383.768116         102.0   \n",
      "2       2     146.647558             33.0  384.762208         106.0   \n",
      "3       3     123.437799             32.0  350.495215         109.0   \n",
      "4       4     169.139303             30.0  400.703980          95.5   \n",
      "\n",
      "   replies_mean  replies_median  \n",
      "0      6.805415             2.0  \n",
      "1      7.741546             2.0  \n",
      "2      8.598726             2.0  \n",
      "3      7.720096             2.0  \n",
      "4      8.114428             2.0  \n",
      "\n",
      "Topic Descriptions:\n",
      "Topic 0: democratic candidate vote way time district gop new national thing\n",
      "Topic 1: post think state thread criminal people end money want mueller\n",
      "Topic 2: unroll comment anon related current drop say politico demand el\n",
      "Topic 3: say work asked mean answer tell witness year fact president\n",
      "Topic 4: le people social right la incumbent medium bio actually look\n",
      "Loading data from C:/Users/Jackd/Downloads/archive (1)/twentyfive_thirty.csv\n",
      "Topic 0: want mean people thing started make knew really facebook year\n",
      "Topic 1: path new le la time come people police idea story\n",
      "Topic 2: unroll le say post use right anon people woman used\n",
      "Topic 3: people said great fbi le left attack moore making brain\n",
      "Topic 4: polling russia obama told think need look white poster thing\n",
      "\n",
      "Engagement Summary for Thread 2530:\n",
      "   Topic_  retweets_mean  retweets_median  likes_mean  likes_median  \\\n",
      "0       0      64.360190             17.0  201.496051          80.0   \n",
      "1       1     118.823817             15.0  306.962480          50.0   \n",
      "2       2     112.074856             19.0  280.191939          63.0   \n",
      "3       3     113.449438             29.0  322.436330          94.5   \n",
      "4       4     134.286299             21.0  327.803681          62.0   \n",
      "\n",
      "   replies_mean  replies_median  \n",
      "0      4.001580             2.0  \n",
      "1      5.861338             2.0  \n",
      "2      5.854127             2.0  \n",
      "3      5.312734             2.0  \n",
      "4      6.734151             1.0  \n",
      "\n",
      "Topic Descriptions:\n",
      "Topic 0: want mean people thing started make knew really facebook year\n",
      "Topic 1: path new le la time come people police idea story\n",
      "Topic 2: unroll le say post use right anon people woman used\n",
      "Topic 3: people said great fbi le left attack moore making brain\n",
      "Topic 4: polling russia obama told think need look white poster thing\n",
      "Loading data from C:/Users/Jackd/Downloads/archive (1)/fifteen_twenty.csv\n",
      "Topic 0: le thread et la version story running year important read\n",
      "Topic 1: want think tell people say million control russian fbi obama\n",
      "Topic 2: democratic candidate la le du claim water devil et incumbent\n",
      "Topic 3: people big company thing make tweet build use writing social\n",
      "Topic 4: student bible unroll hosted gop try primary people great june\n",
      "\n",
      "Engagement Summary for Thread 1520:\n",
      "   Topic_  retweets_mean  retweets_median  likes_mean  likes_median  \\\n",
      "0       0      99.210648             14.0  199.196759          32.5   \n",
      "1       1     210.730104             22.0  421.771626          57.0   \n",
      "2       2      91.242075             11.0  225.239193          31.0   \n",
      "3       3      98.819277             25.5  288.409639          71.5   \n",
      "4       4      91.323782             12.0  275.106017          33.0   \n",
      "\n",
      "   replies_mean  replies_median  \n",
      "0      4.909722             1.0  \n",
      "1     11.141869             2.0  \n",
      "2      6.628242             2.0  \n",
      "3      4.602410             2.0  \n",
      "4      3.449857             1.0  \n",
      "\n",
      "Topic Descriptions:\n",
      "Topic 0: le thread et la version story running year important read\n",
      "Topic 1: want think tell people say million control russian fbi obama\n",
      "Topic 2: democratic candidate la le du claim water devil et incumbent\n",
      "Topic 3: people big company thing make tweet build use writing social\n",
      "Topic 4: student bible unroll hosted gop try primary people great june\n",
      "Loading data from C:/Users/Jackd/Downloads/archive (1)/five_ten.csv\n",
      "Topic 0: indictment believe law sealed big va make arrested pruitt reality\n",
      "Topic 1: people thread report cia work committee state end gun brings\n",
      "Topic 2: new post watch try link fbi step feel medium need\n",
      "Topic 3: say stage nra march medium democrat la think candidate issue\n",
      "Topic 4: investigation unroll look archive brings time historical military need anons\n",
      "\n",
      "Engagement Summary for Thread 0510:\n",
      "   Topic_  retweets_mean  retweets_median  likes_mean  likes_median  \\\n",
      "0       0      77.681818             16.5  121.884298          29.5   \n",
      "1       1     102.980769             25.5  155.448718          44.0   \n",
      "2       2      78.217949             25.0  135.076923          37.0   \n",
      "3       3      83.296875             29.0  132.062500          49.5   \n",
      "4       4      73.741935             22.0  125.685484          40.0   \n",
      "\n",
      "   replies_mean  replies_median  \n",
      "0      5.330579             2.0  \n",
      "1      6.557692             3.0  \n",
      "2      7.371795             2.0  \n",
      "3      6.703125             2.0  \n",
      "4      6.991935             3.0  \n",
      "\n",
      "Topic Descriptions:\n",
      "Topic 0: indictment believe law sealed big va make arrested pruitt reality\n",
      "Topic 1: people thread report cia work committee state end gun brings\n",
      "Topic 2: new post watch try link fbi step feel medium need\n",
      "Topic 3: say stage nra march medium democrat la think candidate issue\n",
      "Topic 4: investigation unroll look archive brings time historical military need anons\n",
      "Loading data from C:/Users/Jackd/Downloads/archive (1)/ten_fifteen.csv\n",
      "Topic 0: thought told talk human remember country people prince way money\n",
      "Topic 1: think really child election thing medium thread social msm national\n",
      "Topic 2: unroll declassified want portion called new work facebook good evidence\n",
      "Topic 3: posted fbi got post world president uk cohen miss ask\n",
      "Topic 4: people mueller care law tell need let want relationship time\n",
      "\n",
      "Engagement Summary for Thread 1015:\n",
      "   Topic_  retweets_mean  retweets_median  likes_mean  likes_median  \\\n",
      "0       0      77.406557             13.0  142.881967          25.0   \n",
      "1       1      70.117904             14.0  146.768559          37.0   \n",
      "2       2     125.389908             20.0  260.935780          45.5   \n",
      "3       3     123.477366             28.0  230.263374          49.0   \n",
      "4       4      87.569170             24.0  194.355731          47.0   \n",
      "\n",
      "   replies_mean  replies_median  \n",
      "0      6.698361             2.0  \n",
      "1      5.253275             2.0  \n",
      "2      9.114679             2.0  \n",
      "3      8.337449             2.0  \n",
      "4      7.114625             2.0  \n",
      "\n",
      "Topic Descriptions:\n",
      "Topic 0: thought told talk human remember country people prince way money\n",
      "Topic 1: think really child election thing medium thread social msm national\n",
      "Topic 2: unroll declassified want portion called new work facebook good evidence\n",
      "Topic 3: posted fbi got post world president uk cohen miss ask\n",
      "Topic 4: people mueller care law tell need let want relationship time\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "\n",
    "# Custom stopwords setup\n",
    "custom_stop_words = stopwords.words('english') + ['https', 'amp', 'like', 'know', 'll', 've', 'don', 'trump']\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.lower().split() if word.isalpha() and word not in custom_stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_descriptions = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        topic_descriptions[topic_idx] = top_words\n",
    "        print(\"Topic {}: {}\".format(topic_idx, top_words))\n",
    "    return topic_descriptions\n",
    "\n",
    "def process_thread(file_path, n_topics=5, no_top_words=10):\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "    # Check if 'text' column exists\n",
    "    if 'text' not in df.columns:\n",
    "        print(f\"No 'text' column in {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Preprocess the text data\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Text vectorization with TF-IDF\n",
    "    vect = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')\n",
    "    dtm = vect.fit_transform(df['text'])\n",
    "\n",
    "    # Apply LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    # Display top words for each topic\n",
    "    topic_descriptions = display_topics(lda, vect.get_feature_names_out(), no_top_words)\n",
    "\n",
    "    # Assign the dominant topic to each document\n",
    "    df['Topic'] = lda.transform(dtm).argmax(axis=1)\n",
    "\n",
    "    # Convert 'timestamp' to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Group by Topic and calculate engagement metrics\n",
    "    grouped = df.groupby('Topic').agg({\n",
    "        'retweets': ['mean', 'median'],\n",
    "        'likes': ['mean', 'median'],\n",
    "        'replies': ['mean', 'median']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten multi-index columns for easier access\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "    return grouped, topic_descriptions\n",
    "\n",
    "# Define paths to the CSV files and process each thread\n",
    "threads = {\n",
    "    \"2025\": \"C:/Users/Jackd/Downloads/archive (1)/twenty_twentyfive.csv\",\n",
    "    \"2530\": \"C:/Users/Jackd/Downloads/archive (1)/twentyfive_thirty.csv\",\n",
    "    \"1520\": \"C:/Users/Jackd/Downloads/archive (1)/fifteen_twenty.csv\",\n",
    "    \"0510\": \"C:/Users/Jackd/Downloads/archive (1)/five_ten.csv\",\n",
    "    \"1015\": \"C:/Users/Jackd/Downloads/archive (1)/ten_fifteen.csv\"\n",
    "}\n",
    "\n",
    "for thread_id, file_path in threads.items():\n",
    "    result = process_thread(file_path)\n",
    "    if result:\n",
    "        engagement_summary, topics = result\n",
    "        print(f\"\\nEngagement Summary for Thread {thread_id}:\")\n",
    "        print(engagement_summary)\n",
    "        print(\"\\nTopic Descriptions:\")\n",
    "        for idx, desc in topics.items():\n",
    "            print(f\"Topic {idx}: {desc}\")\n",
    "    else:\n",
    "        print(f\"Skipped {thread_id} due to missing 'text' column or other issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2ca5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
